{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SyntheticTGAN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"U9A5D8gN3z67"},"source":["import numpy as np\n","import pandas as pd\n","\n","from sklearn.exceptions import ConvergenceWarning\n","from sklearn.mixture import BayesianGaussianMixture\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.utils._testing import ignore_warnings\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import *\n","from tensorflow.keras import *\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras import activations\n","from tensorflow.keras import layers\n","from tensorflow import keras\n","\n","import time\n","\n","from google.colab import drive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YLb68RyFV9WH","executionInfo":{"status":"ok","timestamp":1636216008716,"user_tz":-60,"elapsed":27251,"user":{"displayName":"Aurel Steve AVOMO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggi9E6HFsZM1Bm-x69aI0UuldSv9ATTh5Lrnn9KgQ=s64","userId":"07271015394144128293"}},"outputId":"c070d697-e760-42b7-d40d-d3e35ab3ddde"},"source":["drive.mount(\"/content/drive\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"uOi4-IoNOm-t"},"source":["class DataTransformer(object):\n","    \"\"\"Data Transformer.\n","\n","    Model continuous columns with a BayesianGMM and normalized to a scalar\n","    [0, 1] and a vector.\n","    Discrete columns are encoded using a scikit-learn OneHotEncoder.\n","\n","    Args:\n","        n_cluster (int):\n","            Number of modes.\n","        epsilon (float):\n","            Epsilon value.\n","    \"\"\"\n","\n","    def __init__(self, n_clusters=10, epsilon=0.005):\n","        self.n_clusters = n_clusters\n","        self.epsilon = epsilon\n","\n","    @ignore_warnings(category=ConvergenceWarning)\n","    def _fit_continuous(self, column, data):\n","        gm = BayesianGaussianMixture(\n","            self.n_clusters,\n","            weight_concentration_prior_type='dirichlet_process',\n","            weight_concentration_prior=0.001,\n","            n_init=1\n","        )\n","        gm.fit(data)\n","        components = gm.weights_ > self.epsilon\n","        num_components = components.sum()\n","\n","        return {\n","            'name': column,\n","            'model': gm,\n","            'components': components,\n","            'output_info': [(1, 'tanh'), (num_components, 'softmax')],\n","            'output_dimensions': 1 + num_components,\n","        }\n","\n","    @staticmethod\n","    def _fit_discrete(column, data):\n","        ohe = OneHotEncoder(sparse=False)\n","        ohe.fit(data)\n","        categories = len(ohe.categories_[0])\n","\n","        return {\n","            'name': column,\n","            'encoder': ohe,\n","            'output_info': [(categories, 'softmax')],\n","            'output_dimensions': categories\n","        }\n","\n","    def fit(self, data, discrete_columns=()):\n","        self.output_info = []\n","        self.output_dimensions = 0\n","\n","        if not isinstance(data, pd.DataFrame):\n","            self.dataframe = False\n","            data = pd.DataFrame(data)\n","        else:\n","            self.dataframe = True\n","\n","        self.meta = []\n","        for column in data.columns:\n","            column_data = data[[column]].values\n","            if column in discrete_columns:\n","                meta = self._fit_discrete(column, column_data)\n","            else:\n","                meta = self._fit_continuous(column, column_data)\n","\n","            self.output_info += meta['output_info']\n","            self.output_dimensions += meta['output_dimensions']\n","            self.meta.append(meta)\n","\n","    def _transform_continuous(self, column_meta, data):\n","        components = column_meta['components']\n","\n","        model = column_meta['model']\n","\n","        means = model.means_.reshape((1, self.n_clusters))\n","        stds = np.sqrt(model.covariances_).reshape((1, self.n_clusters))\n","        features = (data - means) / (4 * stds)\n","\n","        probs = model.predict_proba(data)\n","        n_opts = components.sum()\n","        features = features[:, components]\n","        probs = probs[:, components]\n","\n","        opt_sel = np.zeros(len(data), dtype='int')\n","        for i in range(len(data)):\n","            pp = probs[i] + 1e-6\n","            pp = pp / pp.sum()\n","            opt_sel[i] = np.random.choice(np.arange(n_opts), p=pp)\n","\n","        idx = np.arange((len(features)))\n","        features = features[idx, opt_sel].reshape([-1, 1])\n","        features = np.clip(features, -.99, .99)\n","\n","        probs_onehot = np.zeros_like(probs)\n","        probs_onehot[np.arange(len(probs)), opt_sel] = 1\n","  \n","        return [features, probs_onehot]\n","\n","    @staticmethod\n","    def _transform_discrete(column_meta, data):\n","        encoder = column_meta['encoder']\n","        return encoder.transform(data)\n","\n","    def transform(self, data):\n","        if not isinstance(data, pd.DataFrame):\n","            data = pd.DataFrame(data)\n","\n","        values = []\n","        for meta in self.meta:\n","            column_data = data[[meta['name']]].values\n","            if 'model' in meta:\n","                values += self._transform_continuous(meta, column_data)\n","            else:\n","                values.append(self._transform_discrete(meta, column_data))\n","\n","        return np.concatenate(values, axis=1).astype(float)\n","        # return values\n","\n","    def _inverse_transform_continuous(self, meta, data, sigma):\n","        model = meta['model']\n","        components = meta['components']\n","\n","        u = data[:, 0]\n","        v = data[:, 1:]\n","\n","        if sigma is not None:\n","            u = np.random.normal(u, sigma)\n","\n","        u = np.clip(u, -1, 1)\n","        v_t = np.ones((len(data), self.n_clusters)) * -100\n","        v_t[:, components] = v\n","        v = v_t\n","        means = model.means_.reshape([-1])\n","        stds = np.sqrt(model.covariances_).reshape([-1])\n","        p_argmax = np.argmax(v, axis=1)\n","        std_t = stds[p_argmax]\n","        mean_t = means[p_argmax]\n","        column = u * 4 * std_t + mean_t\n","\n","        return column\n","\n","    @staticmethod\n","    def _inverse_transform_discrete(meta, data):\n","        encoder = meta['encoder']\n","        return encoder.inverse_transform(data)\n","\n","    def inverse_transform(self, data, sigmas):\n","        start = 0\n","        output = []\n","        column_names = []\n","        for meta in self.meta:\n","            dimensions = meta['output_dimensions']\n","            columns_data = data[:, start:start + dimensions]\n","\n","            if 'model' in meta:\n","                sigma = sigmas[start] if sigmas else None\n","                inverted = self._inverse_transform_continuous(meta, columns_data, sigma)\n","            else:\n","                inverted = self._inverse_transform_discrete(meta, columns_data)\n","\n","            output.append(inverted)\n","            column_names.append(meta['name'])\n","            start += dimensions\n","\n","        output = np.column_stack(output)\n","        if self.dataframe:\n","            output = pd.DataFrame(output, columns=column_names)\n","\n","        return output\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MwxZX1QQWLhz"},"source":["MAIN_PATH = \"/content/drive/MyDrive/bnp_competition/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"abssx3mtOp6p","executionInfo":{"status":"ok","timestamp":1636216034910,"user_tz":-60,"elapsed":813,"user":{"displayName":"Aurel Steve AVOMO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggi9E6HFsZM1Bm-x69aI0UuldSv9ATTh5Lrnn9KgQ=s64","userId":"07271015394144128293"}},"outputId":"a10a53bc-456f-47ce-e2fd-93d0ef013cce"},"source":["train_data = pd.read_csv(MAIN_PATH + \"train.csv\", names = [\"Index1\", \"Index2\", \"Index3\", \"Index4\"])\n","train_data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Index1</th>\n","      <th>Index2</th>\n","      <th>Index3</th>\n","      <th>Index4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.012495</td>\n","      <td>0.011126</td>\n","      <td>0.003252</td>\n","      <td>0.006625</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.011439</td>\n","      <td>0.002691</td>\n","      <td>0.001206</td>\n","      <td>0.006947</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000632</td>\n","      <td>0.007277</td>\n","      <td>0.004049</td>\n","      <td>0.000074</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.017828</td>\n","      <td>0.028210</td>\n","      <td>0.007758</td>\n","      <td>0.007382</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.021115</td>\n","      <td>0.019642</td>\n","      <td>0.009238</td>\n","      <td>0.011499</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     Index1    Index2    Index3    Index4\n","0  0.012495  0.011126  0.003252  0.006625\n","1  0.011439  0.002691  0.001206  0.006947\n","2  0.000632  0.007277  0.004049  0.000074\n","3  0.017828  0.028210  0.007758  0.007382\n","4  0.021115  0.019642  0.009238  0.011499"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"SAK-l5McWRNR"},"source":["transformer = DataTransformer()\n","transformer.fit(train_data)\n","res = transformer.transform(train_data)\n","\n","# transformed_train = transformer.transform(train_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"qp2oz8sPyx8W","executionInfo":{"status":"ok","timestamp":1636216042458,"user_tz":-60,"elapsed":19,"user":{"displayName":"Aurel Steve AVOMO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggi9E6HFsZM1Bm-x69aI0UuldSv9ATTh5Lrnn9KgQ=s64","userId":"07271015394144128293"}},"outputId":"565e8a72-0b73-4044-ef8e-f811f11c98bb"},"source":["pd.DataFrame(res)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.508719</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.534733</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>-0.111512</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.150406</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.434615</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>-0.140180</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>-0.269221</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.175812</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.323417</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.226823</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>-0.050096</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-0.366724</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.216074</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.047855</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.235878</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-0.182315</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.360324</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.352918</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.349950</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.023166</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>741</th>\n","      <td>-0.231789</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.351280</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>-0.059507</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.030481</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>742</th>\n","      <td>-0.016814</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.320852</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-0.126531</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-0.215695</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>743</th>\n","      <td>0.171173</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>-0.457288</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.177615</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-0.288847</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>744</th>\n","      <td>-0.129551</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>-0.253075</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.225299</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-0.239970</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>745</th>\n","      <td>-0.045689</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.178618</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.223697</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-0.073628</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>746 rows × 25 columns</p>\n","</div>"],"text/plain":["           0    1    2    3    4    5   ...        19   20   21   22   23   24\n","0    0.508719  0.0  0.0  0.0  0.0  0.0  ...  0.150406  0.0  0.0  0.0  0.0  1.0\n","1    0.434615  0.0  0.0  0.0  0.0  0.0  ...  0.175812  0.0  0.0  0.0  0.0  1.0\n","2   -0.323417  0.0  0.0  0.0  0.0  0.0  ... -0.366724  0.0  0.0  0.0  0.0  1.0\n","3    0.216074  0.0  1.0  0.0  0.0  0.0  ... -0.182315  1.0  0.0  0.0  0.0  0.0\n","4    0.360324  0.0  1.0  0.0  0.0  0.0  ...  0.023166  1.0  0.0  0.0  0.0  0.0\n","..        ...  ...  ...  ...  ...  ...  ...       ...  ...  ...  ...  ...  ...\n","741 -0.231789  0.0  0.0  0.0  0.0  0.0  ...  0.030481  0.0  0.0  0.0  0.0  1.0\n","742 -0.016814  0.0  0.0  0.0  0.0  0.0  ... -0.215695  0.0  0.0  0.0  0.0  1.0\n","743  0.171173  0.0  0.0  0.0  0.0  0.0  ... -0.288847  0.0  0.0  0.0  1.0  0.0\n","744 -0.129551  0.0  0.0  0.0  0.0  0.0  ... -0.239970  0.0  0.0  0.0  0.0  1.0\n","745 -0.045689  0.0  0.0  0.0  0.0  0.0  ... -0.073628  1.0  0.0  0.0  0.0  0.0\n","\n","[746 rows x 25 columns]"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"Mjncoqiwo_8P"},"source":["u = [res[i] for i in range(8) if i%2 == 1]\n","v = [res[i] for i in range(8) if i%2 == 0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D_hjgfABq1JF","executionInfo":{"status":"ok","timestamp":1636216052583,"user_tz":-60,"elapsed":371,"user":{"displayName":"Aurel Steve AVOMO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggi9E6HFsZM1Bm-x69aI0UuldSv9ATTh5Lrnn9KgQ=s64","userId":"07271015394144128293"}},"outputId":"594f8be8-de10-454e-cad7-9ecbb472c797"},"source":["u[3].shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(25,)"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"8J34WtjzsC3K"},"source":["# Other Experiments"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QtBP6Ip7pfa9","executionInfo":{"status":"ok","timestamp":1636216056263,"user_tz":-60,"elapsed":371,"user":{"displayName":"Aurel Steve AVOMO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggi9E6HFsZM1Bm-x69aI0UuldSv9ATTh5Lrnn9KgQ=s64","userId":"07271015394144128293"}},"outputId":"232b2d93-b2c0-41aa-d995-55357f72b799"},"source":["for item in u:\n","  if(item.shape[-1] != 5):\n","    print(item.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(25,)\n","(25,)\n","(25,)\n","(25,)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":162},"id":"G0WzX85GkfSl","executionInfo":{"status":"error","timestamp":1636112544450,"user_tz":-60,"elapsed":15,"user":{"displayName":"Brown Ebouky","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHg6Xs86SN7upnGqq_nl9lmFR5ZOOmcP9JSAf2qQ=s64","userId":"07069399121465212210"}},"outputId":"01dce69d-da21-4206-cea0-d6ef336b4f2a"},"source":["transformed_train[5]"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-125-42f11c51107d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransformed_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'transformed_train' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":162},"id":"AQ14ZaobiRNN","executionInfo":{"status":"error","timestamp":1636112544751,"user_tz":-60,"elapsed":315,"user":{"displayName":"Brown Ebouky","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHg6Xs86SN7upnGqq_nl9lmFR5ZOOmcP9JSAf2qQ=s64","userId":"07069399121465212210"}},"outputId":"af87f994-04aa-4ee1-9e55-25be99545f02"},"source":["transformed_train.shape"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-126-bdb2bac63460>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransformed_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'transformed_train' is not defined"]}]},{"cell_type":"code","metadata":{"id":"QILDrHhdW_Wa"},"source":["tmp = transformer.inverse_transform(transformed_train, None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GUUK7Fs7eo_g"},"source":["class Linear(keras.layers.Layer):\n","    def __init__(self, input_dim=32, units=32):\n","        super(Linear, self).__init__()\n","        self.w = self.add_weight(\n","            shape=(input_dim, units), initializer=\"random_normal\", trainable=True\n","        )\n","        self.b = self.add_weight(shape=(units,), initializer=\"zeros\", trainable=True)\n","\n","    def call(self, inputs):\n","        return tf.matmul(inputs, self.w) + self.b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g7As3qOVdZgz"},"source":["class Residual(keras.layers.Layer):\n","  def __init__(self, i, o):\n","        super(Residual, self).__init__()\n","        self.fc = Linear(i, o)\n","        self.bn = BatchNormalization()\n","        self.relu = layers.Activation(activations.relu)\n","\n","  def call(self, input):\n","    x = self.fc(input)\n","    x = self.bn(x)\n","    x = self.relu(x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vxEbFWC-Z3Wh"},"source":["class Generator():\n","  def __init__(self, embedding_dim, gen_dims, data_dim, latent_dim):\n","      dim = embedding_dim\n","      self.seq = Sequential()\n","      for item in list(gen_dims):\n","          self.seq.add(Residual(dim, item))\n","          dim += item\n","\n","      self.seq.add(Linear(dim, data_dim))\n","      noise = Input(shape=(latent_dim,))\n","\n","      res = self.seq(noise)\n","\n","      return Model(noise, res)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MqqEYr4BwNGT"},"source":["class Generator(keras.Model):\n","  def __init__(self, n_units, n_z, output_length, n_x, n_u, n_v):\n","    super(Generator, self).__init__()\n","    self.alpha = tf.Variable(shape = (1,output_length,), \n","                             initial_value= tf.random.normal(shape=(1,output_length,)))\n","    self.lstm = LSTM(n_units, activation='relu', return_state=True)\n","    self.n_z = n_z\n","    self.n_units = n_units\n","    self.output_length =  output_length\n","    self.n_x = n_x\n","    self.n_v = n_v\n","    self.n_u = n_u\n","    self.projector_f = Dense(self.n_x, activation=\"tanh\")\n","    self.projector_v = [Dense(i, activation='tanh') for i in n_v]\n","    self.projector_u = [Dense(i, activation='tanh') for i in n_u]\n","    \n","\n","\n","  def give_noise(self):\n","    return tf.random.normal(shape=(1, 1, self.n_z,),)\n","\n","  def give_attention(self, h,):\n","    n_dim = h.shape[-2]\n","    alpha_pr = self.alpha[:n_dim]\n","    alpha_pr = tf.nn.softmax(alpha_pr)\n","    \n","    output = tf.math.reduce_sum(alpha_pr*h, axis=-2)\n","\n","    return output\n","\n","  def call(self, x):\n","    a = tf.zeros(shape=(1, 1, self.n_units,))\n","    z = self.give_noise()\n","    input = tf.concat([z, tf.reshape(x[0], shape=(1, 1, -1)), a], axis = -1)\n","    output_h = np.random.randn(self.output_length, self.n_units)\n","    output_v = [np.random.randn(i) for i in self.n_v]\n","    output_u = [np.random.randn(i) for i in self.n_u]\n","\n","    h = tf.zeros(shape=(1, self.n_units,))\n","    c = tf.zeros(shape=(1, self.n_units,))\n","    flag = True\n","\n","    u_cur, v_cur = 0, 0\n","    for i in range(self.output_length):\n","        _, h, c = self.lstm(input, initial_state=[h, c])\n","        # print(h)\n","        # print(output_h[i])\n","        # tmp = output_h.numpy()\n","        # tmp[i] = h.numpy()\n","        # output_h = tf.convert_to_tensor(tmp)\n","        output_h[i] = h.numpy()\n","        f = self.projector_f(h)\n","        f = tf.reshape(f, shape=(1, 1, -1))\n","        a = self.give_attention(output_h[:i+1]) \n","        a = tf.reshape(a, shape=(1, 1, -1))\n","        # print(z.shape, f.shape, a.shape)\n","        input = tf.concat([z, f, a], axis = -1)\n","\n","        # print(flag)\n","        if(flag):\n","          output_v[i//2] = self.projector_v[v_cur](f)\n","          print(output_v[i//2].shape)\n","          v_cur = v_cur + 1\n","        else:\n","          output_u[i//2] = self.projector_u[u_cur](f)\n","          u_cur = u_cur + 1\n","        \n","        flag = not flag\n","\n","    return (output_u, output_v)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HDCCqIS_NN_4"},"source":["class DiversityLayer(Layer):\n","\n","    def __init__(self, hidden_dim, alpha_leaky):\n","      super(DiversityLayer, self).__init__()\n","      self.diversity_features_projection = Dense(hidden_dim)\n","      self.layer = Sequential([Dense(hidden_dim), LeakyReLU(alpha=alpha_leaky)])\n","    \n","    def call(self, x):\n","      output = x + self._diversity(x)\n","      return self.layer(output)\n","\n","    def _diversity(self, X):\n","      output = self.diversity_features_projection(X)\n","      n = X.shape[0]\n","      results = []\n","      for i in range(n):\n","        xi = tf.stack([X[i]] * n, axis = 0)\n","        tmp = tf.norm(xi - output, ord=1, axis = 0)\n","        results.append(tmp)\n","      return tf.stack(results, axis = 0)\n","\n","class Discriminator(keras.Model):  \n","\n","\n","  def __init__(self, number_div_layers, hidden_dim, diversity_input_dim, diversity_output_dim, alpha_leaky=0.03):\n","    super(Discriminator, self).__init__()\n","\n","    self.layer1 = Sequential([Dense(hidden_dim), LeakyReLU(alpha=alpha_leaky)])\n","\n","    self.diversity_layers =  Sequential([DiversityLayer(hidden_dim, alpha_leaky) for _ in range(number_div_layers)])\n","\n","    self.output_projector = Dense(1, activation='sigmoid')\n","\n","  \n","  def call(self, x):\n","    output = self.layer1(x)\n","    output = self.diversity_layers(output)\n","    output = self.output_projector(output)\n","\n","    return output\n","  \n","\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zmoQv0JYXXx4"},"source":["def generator_loss(discriminator, generator, u_gen, v_gen, u,):\n","  u_gen = tf.reshape(u_gen, shape=(u_gen.shape[0], -1))\n","  v_gen = tf.reshape(v_gen, shape=(v_gen.shape[0], -1))\n","\n","  uv_flat = tf.concat([u_gen, v_gen], axis = -1)\n","\n","  out_dis = discriminator(uv_flat)\n","\n","  loss = - tf.math.reduce_mean((tf.math.log(out_dis))) \n","  loss +=  tf.losses.KLDivergence(u, u_gen)\n","  \n","  return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"phrD4GWObbsG"},"source":["def discriminator_loss(discriminator, generator, x, u, v, u_gen, v_gen):\n","  # u_gen, v_gen = generator(x)\n","  u_gen = tf.reshape(u_gen, shape=(u_gen.shape[0], -1))\n","  v_gen = tf.reshape(v_gen, shape=(v_gen.shape[0], -1))\n","\n","  uv_gen_flat = tf.concat([u_gen, v_gen], axis = -1)\n","\n","  out_gen_dis = discriminator(uv_gen_flat)\n","\n","  loss = tf.math.reduce_mean((tf.math.log(out_gen_dis))) \n","\n","  u = tf.reshape(u, shape=(u.shape[0], -1))\n","  v = tf.reshape(v, shape=(v.shape[0], -1))\n","\n","  uv_flat = tf.concat([u, v], axis = -1)\n","  out_dis = discriminator(uv_flat)\n","\n","  loss -=  tf.math.reduce_mean(tf.math.log(out_dis))\n","\n","  return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JGb0F3Tbhllw"},"source":["generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n","discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AUnao6xHs-Ss"},"source":["generator_model = Generator(n_units=8, n_z = 4, output_length=8, n_x =4, n_u = [6, 7, 7, 4], n_v=[1, 1, 1, 1])\n","discriminator_model = Discriminator(number_div_layers=2, hidden_dim=4, diversity_input_dim=4, diversity_output_dim=2, alpha_leaky=0.03)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3FfFi9RiydAL"},"source":["def my_concatenation(u_gen, v_gen):\n","  res = []\n","  for i in range(4):\n","    print(u_gen[i].shape)\n","    res.append(tf.reshape(v_gen[i], shape=(-1,)))\n","    res.extend(tf.reshape(u_gen[i], shape=(-1,)))\n","\n","  res = tf.concat(res, axis=-1)\n","  return res"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uAbI-4y5h0ho"},"source":["# Notice the use of `tf.function`\n","# This annotation causes the function to be \"compiled\".\n","\n","def train_step():\n","    # noise = tf.random.normal([BATCH_SIZE, noise_dim])\n","\n","    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","      u_gen, v_gen = generator_model(np.random.normal(0.0, 0.1, (32,1,4)))\n","\n","      # real_output = discriminator_model(res)\n","      # print(u_gen.shape, v_gen.shape)\n","      fake_output = discriminator_model(my_concatenation(u_gen, v_gen))\n","      \n","      gen_loss = generator_loss(discriminator, generator, u_gen, v_gen, u)\n","      disc_loss = discriminator_loss(discriminator, generator, x, u, v, u_gen, v_gen)\n","\n","    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n","    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n","\n","    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n","    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GbYP0Cwo4uiZ"},"source":["def train(epochs):\n","  for epoch in range(epochs):\n","    start = time.time()\n","\n","    train_step()\n","    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":307},"id":"BBvkHi_l5S6g","executionInfo":{"status":"error","timestamp":1636216115308,"user_tz":-60,"elapsed":434,"user":{"displayName":"Aurel Steve AVOMO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggi9E6HFsZM1Bm-x69aI0UuldSv9ATTh5Lrnn9KgQ=s64","userId":"07271015394144128293"}},"outputId":"6688bcce-9009-4029-93c2-eb87128587f3"},"source":["train(5)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-bcce4a81000f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-21-0a9edcacc716>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Time for epoch {} is {} sec'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-7f1bac885cf7>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgen_tape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdisc_tape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m       \u001b[0mu_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0;31m# real_output = discriminator_model(res)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'generator_model' is not defined"]}]},{"cell_type":"code","metadata":{"id":"mFyynbeFCHri"},"source":["tmp = tf.random.uniform(shape=(8,8))\n","x = tf.random.uniform(shape=(1,8))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KTMAP7YUCT-O","colab":{"base_uri":"https://localhost:8080/","height":162},"executionInfo":{"status":"error","timestamp":1636112546882,"user_tz":-60,"elapsed":5,"user":{"displayName":"Brown Ebouky","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHg6Xs86SN7upnGqq_nl9lmFR5ZOOmcP9JSAf2qQ=s64","userId":"07069399121465212210"}},"outputId":"5685eb60-c320-414d-dfe6-0dc9df16b90a"},"source":["tmp[0] = x"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-139-69fcc8806737>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"]}]},{"cell_type":"code","metadata":{"id":"ioYXbjGghbSo","colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"status":"error","timestamp":1636112547244,"user_tz":-60,"elapsed":36,"user":{"displayName":"Brown Ebouky","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHg6Xs86SN7upnGqq_nl9lmFR5ZOOmcP9JSAf2qQ=s64","userId":"07069399121465212210"}},"outputId":"62e3c01e-301d-46d7-cfd9-42b13634b669"},"source":["gen_model = Generator(8, 2, 8, 4, 5, 1)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-140-65e3d438032c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgen_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-129-751a7a91690b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_units, n_z, output_length, n_x, n_u, n_v)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_u\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojector_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojector_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn_v\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojector_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn_u\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"]}]},{"cell_type":"code","metadata":{"id":"8Y9jinVJ_FBI"},"source":["x = tf.random.normal(shape=(1,1,4))\n","y = gen_model.call(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1I0Pmo2AFXZs"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ydnW-7CplltF"},"source":["discriminator = Discriminator(2, 4, 4, 4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-4Km2OjjiWOF","executionInfo":{"status":"ok","timestamp":1636112547988,"user_tz":-60,"elapsed":340,"user":{"displayName":"Brown Ebouky","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHg6Xs86SN7upnGqq_nl9lmFR5ZOOmcP9JSAf2qQ=s64","userId":"07069399121465212210"}},"outputId":"beb287ea-dc70-40ce-cc46-4d708a5ce69c"},"source":["y = tf.random.normal(shape=(1,4))\n","discriminator(y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.77307785]], dtype=float32)>"]},"metadata":{},"execution_count":142}]},{"cell_type":"code","metadata":{"id":"whCb8hyQWtuB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wequgUgJmahz"},"source":["# Conditional TGAN"]},{"cell_type":"markdown","metadata":{"id":"P6M7UExSryF2"},"source":["## Class"]},{"cell_type":"code","metadata":{"id":"0-DJvpGzmyKY"},"source":["import numpy as np\n","\n","\n","class CTGanSampler(object):\n","    \"\"\"docstring for Sampler.\"\"\"\n","\n","    def __init__(self, data, output_info):\n","        super(CTGanSampler, self).__init__()\n","        self.data = data\n","        self.model = []\n","        self.n = len(data)\n","\n","        st = 0\n","        skip = False\n","        for item in output_info:\n","            if item[1] == 'tanh':\n","                st += item[0]\n","                skip = True\n","            elif item[1] == 'softmax':\n","                if skip:\n","                    skip = False\n","                    st += item[0]\n","                    continue\n","\n","                ed = st + item[0]\n","                tmp = []\n","                for j in range(item[0]):\n","                    tmp.append(np.nonzero(data[:, st + j])[0])\n","\n","                self.model.append(tmp)\n","                st = ed\n","            else:\n","                assert 0\n","\n","        assert st == data.shape[1]\n","\n","    def sample(self, n, col, opt):\n","        if col is None:\n","            idx = np.random.choice(np.arange(self.n), n)\n","            return self.data[idx]\n","\n","        idx = []\n","        for c, o in zip(col, opt):\n","            idx.append(np.random.choice(self.model[c][o]))\n","\n","        return self.data[idx]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ICfUYiQSmswO"},"source":["import numpy as np\n","\n","def anderson_evaluation(real, generated):\n","    n = len(real)\n","    num_markets = len(real[0])\n","    \n","    u = np.zeros((n, num_markets))\n","    w = np.zeros(num_markets)\n","    \n","    for d in range(num_markets):\n","        sorted_generated = np.sort(generated[:, d])\n","        \n","        for i in range(1, n+1):\n","            u[i-1][d] = (len(real[:, d][real[:, d]<=sorted_generated[i-1]]) +1) / (n+2)\n","        \n","        sum_logs = 0\n","        for i in range(1, n+1):\n","            sum_logs += (2*i-1)*(np.log(u[i-1][d]) + np.log(1-u[n-i][d]))\n","        \n","        w[d] = -1 * (n + sum_logs/n)\n","    \n","    return np.mean(w)\n","\n","def kendall_evaluation(real, generated):\n","    n = len(real)\n","    d = len(real[0])\n","    \n","    z_real = np.zeros(n)\n","    z_generated = np.zeros(n)\n","    \n","    for i in range(n):\n","        arr_real = np.tile(real[i], (n-1, 1)) # array where we repeat the row [X_i^1, ..., X_i^d] n-i times\n","        arr2_real = np.delete(real, (i), axis=0)\n","        # we extract the max of each column of the difference\n","        # useful because the condition a1<b1 and a2<b2 and... ==> a1-b1<0 and a2-b2<0 and... can be resumed as max(ai-bi)<0\n","        result_real = (arr2_real - arr_real).max(axis=1)\n","        z_real[i] = len(result_real[result_real<0])\n","\n","        arr_generated = np.tile(generated[i], (n-1, 1)) # array where we repeat the row [X_i^1, ..., X_i^d] n-i times\n","        arr2_generated = np.delete(generated, (i), axis=0)\n","        result_generated = (arr2_generated - arr_generated).max(axis=1)\n","        z_generated[i] = len(result_generated[result_generated<0])\n","        \n","    z_real /= (n-1)\n","    z_generated /= (n-1)\n","    \n","    z_real = np.sort(z_real)\n","    z_generated = np.sort(z_generated)\n","    \n","    return np.mean(np.abs(z_real - z_generated))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TYH0J_8KnROr"},"source":["import torch\n","from torch.nn import BatchNorm1d, Dropout, LeakyReLU, Linear, Module, ReLU, Sequential\n","\n","\n","class Discriminator(Module):\n","\n","    def calc_gradient_penalty(self, real_data, fake_data, device='cpu', pac=10, lambda_=10):\n","\n","        alpha = torch.rand(real_data.size(0) // pac, 1, 1, device=device)\n","        alpha = alpha.repeat(1, pac, real_data.size(1))\n","        alpha = alpha.view(-1, real_data.size(1))\n","\n","        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n","\n","        disc_interpolates = self(interpolates)\n","\n","        gradients = torch.autograd.grad(\n","            outputs=disc_interpolates, inputs=interpolates,\n","            grad_outputs=torch.ones(disc_interpolates.size(), device=device),\n","            create_graph=True, retain_graph=True, only_inputs=True\n","        )[0]\n","\n","        gradient_penalty = ((\n","            gradients.view(-1, pac * real_data.size(1)).norm(2, dim=1) - 1\n","        ) ** 2).mean() * lambda_\n","\n","        return gradient_penalty\n","\n","    def __init__(self, input_dim, dis_dims, pack=10):\n","        super(Discriminator, self).__init__()\n","        dim = input_dim * pack\n","        self.pack = pack\n","        self.packdim = dim\n","        seq = []\n","        for item in list(dis_dims):\n","            seq += [Linear(dim, item), LeakyReLU(0.2), Dropout(0.5)]\n","            dim = item\n","\n","        seq += [Linear(dim, 1)]\n","        self.seq = Sequential(*seq)\n","\n","    def forward(self, input):\n","        assert input.size()[0] % self.pack == 0\n","        return self.seq(input.view(-1, self.packdim))\n","\n","\n","class Residual(Module):\n","    def __init__(self, i, o):\n","        super(Residual, self).__init__()\n","        self.fc = Linear(i, o)\n","        self.bn = BatchNorm1d(o)\n","        self.relu = LeakyReLU()\n","\n","    def forward(self, input):\n","        out = self.fc(input)\n","        out = self.bn(out)\n","        out = self.relu(out)\n","        return torch.cat([out, input], dim=1)\n","\n","\n","class Generator(Module):\n","    def __init__(self, embedding_dim, gen_dims, data_dim):\n","        super(Generator, self).__init__()\n","        dim = embedding_dim\n","        seq = []\n","        for item in list(gen_dims):\n","            seq += [Residual(dim, item)]\n","            dim += item\n","        seq.append(Linear(dim, data_dim))\n","        self.seq = Sequential(*seq)\n","\n","    def forward(self, input):\n","        data = self.seq(input)\n","        return data\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JQ_3eT36nafp"},"source":["import numpy as np\n","\n","\n","class ConditionalGenerator(object):\n","    def __init__(self, data, output_info, log_frequency):\n","        self.model = []\n","\n","        start = 0\n","        skip = False\n","        max_interval = 0\n","        counter = 0\n","        for item in output_info:\n","            if item[1] == 'tanh':\n","                start += item[0]\n","                skip = True\n","                continue\n","\n","            elif item[1] == 'softmax':\n","                if skip:\n","                    skip = False\n","                    start += item[0]\n","                    continue\n","\n","                end = start + item[0]\n","                max_interval = max(max_interval, end - start)\n","                counter += 1\n","                self.model.append(np.argmax(data[:, start:end], axis=-1))\n","                start = end\n","\n","            else:\n","                assert 0\n","\n","        assert start == data.shape[1]\n","\n","        self.interval = []\n","        self.n_col = 0\n","        self.n_opt = 0\n","        skip = False\n","        start = 0\n","        self.p = np.zeros((counter, max_interval))\n","        for item in output_info:\n","            if item[1] == 'tanh':\n","                skip = True\n","                start += item[0]\n","                continue\n","            elif item[1] == 'softmax':\n","                if skip:\n","                    start += item[0]\n","                    skip = False\n","                    continue\n","                end = start + item[0]\n","                tmp = np.sum(data[:, start:end], axis=0)\n","                if log_frequency:\n","                    tmp = np.log(tmp + 1)\n","                tmp = tmp / np.sum(tmp)\n","                self.p[self.n_col, :item[0]] = tmp\n","                self.interval.append((self.n_opt, item[0]))\n","                self.n_opt += item[0]\n","                self.n_col += 1\n","                start = end\n","            else:\n","                assert 0\n","\n","        self.interval = np.asarray(self.interval)\n","\n","    def random_choice_prob_index(self, idx):\n","        a = self.p[idx]\n","        r = np.expand_dims(np.random.rand(a.shape[0]), axis=1)\n","        return (a.cumsum(axis=1) > r).argmax(axis=1)\n","\n","    def sample(self, batch):\n","        if self.n_col == 0:\n","            return None\n","\n","        batch = batch\n","        idx = np.random.choice(np.arange(self.n_col), batch)\n","\n","        vec1 = np.zeros((batch, self.n_opt), dtype='float32')\n","        mask1 = np.zeros((batch, self.n_col), dtype='float32')\n","        mask1[np.arange(batch), idx] = 1\n","        opt1prime = self.random_choice_prob_index(idx)\n","        opt1 = self.interval[idx, 0] + opt1prime\n","        vec1[np.arange(batch), opt1] = 1\n","\n","        return vec1, mask1, idx, opt1prime\n","\n","    def sample_zero(self, batch):\n","        if self.n_col == 0:\n","            return None\n","\n","        vec = np.zeros((batch, self.n_opt), dtype='float32')\n","        idx = np.random.choice(np.arange(self.n_col), batch)\n","        for i in range(batch):\n","            col = idx[i]\n","            pick = int(np.random.choice(self.model[col]))\n","            vec[i, pick + self.interval[col, 0]] = 1\n","\n","        return vec\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qt0jvOAbmcPQ"},"source":["import numpy as np\n","import torch\n","from torch import optim\n","from torch.nn import functional\n","\n","\n","\n","class EarlyStopping:\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","\n","    def __init__(self, patience=7, verbose=False, delta=0):\n","        \"\"\"\n","        Args:\n","            patience (int): How long to wait after last time validation loss improved.\n","                            Default: 7\n","            verbose (bool): If True, prints a message for each validation loss improvement.\n","                            Default: False\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n","                            Default: 0\n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","\n","    def __call__(self, val_loss):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            #print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.counter = 0\n","\n","\n","class CTGANSynthesizer(object):\n","    \"\"\"Conditional Table GAN Synthesizer.\n","\n","    This is the core class of the CTGAN project, where the different components\n","    are orchestrated together.\n","\n","    For more details about the process, please check the [Modeling Tabular data using\n","    Conditional GAN](https://arxiv.org/abs/1907.00503) paper.\n","\n","    Args:\n","        embedding_dim (int):\n","            Size of the random sample passed to the Generator. Defaults to 128.\n","        gen_dim (tuple or list of ints):\n","            Size of the output samples for each one of the Residuals. A Resiudal Layer\n","            will be created for each one of the values provided. Defaults to (256, 256).\n","        dis_dim (tuple or list of ints):\n","            Size of the output samples for each one of the Discriminator Layers. A Linear Layer\n","            will be created for each one of the values provided. Defaults to (256, 256).\n","        l2scale (float):\n","            Wheight Decay for the Adam Optimizer. Defaults to 1e-6.\n","        batch_size (int):\n","            Number of data samples to process in each step.\n","    \"\"\"\n","\n","    def __init__(self, embedding_dim=128, gen_dim=(256, 256), dis_dim=(256, 256),\n","                 l2scale=1e-6, batch_size=500, patience=25):\n","\n","        self.embedding_dim = embedding_dim\n","        self.gen_dim = gen_dim\n","        self.dis_dim = dis_dim\n","        self.patience = patience\n","        self.l2scale = l2scale\n","        self.batch_size = batch_size\n","        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","    def _apply_activate(self, data):\n","        data_t = []\n","        st = 0\n","        for item in self.transformer.output_info:\n","            if item[1] == 'tanh':\n","                ed = st + item[0]\n","                data_t.append(torch.tanh(data[:, st:ed]))\n","                st = ed\n","            elif item[1] == 'softmax':\n","                ed = st + item[0]\n","                data_t.append(functional.gumbel_softmax(data[:, st:ed], tau=0.2))\n","                st = ed\n","            else:\n","                assert 0\n","\n","        return torch.cat(data_t, dim=1)\n","\n","    def _cond_loss(self, data, c, m):\n","        loss = []\n","        st = 0\n","        st_c = 0\n","        skip = False\n","        for item in self.transformer.output_info:\n","            if item[1] == 'tanh':\n","                st += item[0]\n","                skip = True\n","\n","            elif item[1] == 'softmax':\n","                if skip:\n","                    skip = False\n","                    st += item[0]\n","                    continue\n","\n","                ed = st + item[0]\n","                ed_c = st_c + item[0]\n","                tmp = functional.cross_entropy(\n","                    data[:, st:ed],\n","                    torch.argmax(c[:, st_c:ed_c], dim=1),\n","                    reduction='none'\n","                )\n","                loss.append(tmp)\n","                st = ed\n","                st_c = ed_c\n","\n","            else:\n","                assert 0\n","\n","        loss = torch.stack(loss, dim=1)\n","\n","        return (loss * m).sum() / data.size()[0]\n","\n","    def fit(self, train_data, discrete_columns=(), epochs=300, log_frequency=True):\n","        \"\"\"Fit the CTGAN Synthesizer models to the training data.\n","\n","        Args:\n","            train_data (numpy.ndarray or pandas.DataFrame):\n","                Training Data. It must be a 2-dimensional numpy array or a\n","                pandas.DataFrame.\n","            discrete_columns (list-like):\n","                List of discrete columns to be used to generate the Conditional\n","                Vector. If ``train_data`` is a Numpy array, this list should\n","                contain the integer indices of the columns. Otherwise, if it is\n","                a ``pandas.DataFrame``, this list should contain the column names.\n","            epochs (int):\n","                Number of training epochs. Defaults to 300.\n","            log_frequency (boolean):\n","                Whether to use log frequency of categorical levels in conditional\n","                sampling. Defaults to ``True``.\n","        \"\"\"\n","\n","        self.transformer = DataTransformer()\n","        self.transformer.fit(train_data, discrete_columns)\n","        train_data = self.transformer.transform(train_data)\n","\n","        data_sampler = CTGanSampler(train_data, self.transformer.output_info)\n","\n","        data_dim = self.transformer.output_dimensions\n","        self.cond_generator = ConditionalGenerator(\n","            train_data,\n","            self.transformer.output_info,\n","            log_frequency\n","        )\n","\n","        self.generator = Generator(\n","            self.embedding_dim + self.cond_generator.n_opt,\n","            self.gen_dim,\n","            data_dim\n","        ).to(self.device)\n","\n","        discriminator = Discriminator(\n","            data_dim + self.cond_generator.n_opt,\n","            self.dis_dim\n","        ).to(self.device)\n","\n","        optimizerG = optim.Adam(\n","            self.generator.parameters(), lr=4e-7, betas=(0.5, 0.9),\n","            weight_decay=self.l2scale\n","        )\n","        optimizerD = optim.Adam(discriminator.parameters(), lr=4e-7, betas=(0.5, 0.9))\n","\n","        assert self.batch_size % 2 == 0\n","        #mean = torch.normal(0.01, 0.01, size=(self.batch_size, self.embedding_dim))\n","        mean = torch.zeros(self.batch_size, self.embedding_dim, device=self.device)\n","        std = mean + .01\n","\n","        train_losses = []\n","        early_stopping = EarlyStopping(patience=self.patience, verbose=False)\n","\n","        steps_per_epoch = max(len(train_data) // self.batch_size, 1)\n","        for i in range(epochs):\n","            for id_ in range(steps_per_epoch):\n","                fakez = torch.normal(mean=mean, std=std)\n","\n","                condvec = self.cond_generator.sample(self.batch_size)\n","                if condvec is None:\n","                    c1, m1, col, opt = None, None, None, None\n","                    real = data_sampler.sample(self.batch_size, col, opt)\n","                else:\n","                    c1, m1, col, opt = condvec\n","                    c1 = torch.from_numpy(c1).to(self.device)\n","                    m1 = torch.from_numpy(m1).to(self.device)\n","                    fakez = torch.cat([fakez, c1], dim=1)\n","\n","                    perm = np.arange(self.batch_size)\n","                    np.random.shuffle(perm)\n","                    real = data_sampler.sample(self.batch_size, col[perm], opt[perm])\n","                    c2 = c1[perm]\n","\n","                fake = self.generator(fakez)\n","                fakeact = self._apply_activate(fake)\n","\n","                real = torch.from_numpy(real.astype('float32')).to(self.device)\n","\n","                if c1 is not None:\n","                    fake_cat = torch.cat([fakeact, c1], dim=1)\n","                    real_cat = torch.cat([real, c2], dim=1)\n","                else:\n","                    real_cat = real\n","                    fake_cat = fake\n","\n","                y_fake = discriminator(fake_cat)\n","                y_real = discriminator(real_cat)\n","\n","                pen = discriminator.calc_gradient_penalty(real_cat, fake_cat, self.device)\n","                loss_d = -(torch.mean(y_real) - torch.mean(y_fake))\n","                train_losses.append(loss_d.item())\n","                optimizerD.zero_grad()\n","                pen.backward(retain_graph=True)\n","                loss_d.backward()\n","                optimizerD.step()\n","\n","                fakez = torch.normal(mean=mean, std=std)\n","                condvec = self.cond_generator.sample(self.batch_size)\n","\n","                if condvec is None:\n","                    c1, m1, col, opt = None, None, None, None\n","                else:\n","                    c1, m1, col, opt = condvec\n","                    c1 = torch.from_numpy(c1).to(self.device)\n","                    m1 = torch.from_numpy(m1).to(self.device)\n","                    fakez = torch.cat([fakez, c1], dim=1)\n","\n","                fake = self.generator(fakez)\n","                fakeact = self._apply_activate(fake)\n","\n","                if c1 is not None:\n","                    y_fake = discriminator(torch.cat([fakeact, c1], dim=1))\n","                else:\n","                    y_fake = discriminator(fakeact)\n","\n","                if condvec is None:\n","                    cross_entropy = 0\n","                else:\n","                    cross_entropy = self._cond_loss(fake, c1, m1)\n","\n","                #fake_copy = fakeact #tf.identity(fake)\n","                #fake_copy = fake_copy.detach().numpy()\n","                \n","                #real_copy = real #tf.identity(real)\n","                #real_copy = real_copy.detach().numpy()\n","\n","                #tmp_inv_gen = self.transformer.inverse_transform(fake_copy, None).values\n","                #tmp_inv_real = self.transformer.inverse_transform(real_copy, None).values\n","\n","                \n","                # print(tmp_inv_gen.shape, tmp_inv_real.shape)\n","                loss_g = -torch.mean(y_fake) +  cross_entropy # +  anderson_evaluation(tmp_inv_real, tmp_inv_gen)\n","                train_losses.append(loss_g.item())\n","                optimizerG.zero_grad()\n","                loss_g.backward()\n","                optimizerG.step()\n","            early_stopping(np.average(train_losses))\n","            if early_stopping.early_stop:\n","                print(\"GAN: Early stopping after epochs {}\".format(i))\n","                break\n","            train_losses = []\n","\n","            # print(\"Epoch %d, Loss G: %.4f, Loss D: %.4f\" %\n","            #       (i + 1, loss_g.detach().cpu(), loss_d.detach().cpu()),\n","            #       flush=True)\n","\n","    def sample(self, n):\n","        \"\"\"Sample data similar to the training data.\n","\n","        Args:\n","            n (int):\n","                Number of rows to sample.\n","\n","        Returns:\n","            numpy.ndarray or pandas.DataFrame\n","        \"\"\"\n","\n","        steps = n // self.batch_size + 1\n","        data = []\n","        for i in range(steps):\n","            mean = torch.zeros(self.batch_size, self.embedding_dim)\n","            std = mean + 0.01\n","            fakez = torch.normal(mean=mean, std=std).to(self.device)\n","\n","            condvec = self.cond_generator.sample_zero(self.batch_size)\n","            if condvec is None:\n","                pass\n","            else:\n","                c1 = condvec\n","                c1 = torch.from_numpy(c1).to(self.device)\n","                fakez = torch.cat([fakez, c1], dim=1)\n","\n","            fake = self.generator(fakez)\n","            fakeact = self._apply_activate(fake)\n","            data.append(fakeact.detach().cpu().numpy())\n","\n","        data = np.concatenate(data, axis=0)\n","        data = data[:n]\n","\n","        return self.transformer.inverse_transform(data, None)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bQa8hwq9n6t7"},"source":["import gc\n","from typing import List\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import StratifiedKFold\n","\n","\n","\n","def save_dict_to_file(dic: dict, path: str, save_raw=False) -> None:\n","    \"\"\"\n","    Save dict values into txt file\n","    :param dic: Dict with values\n","    :param path: Path to .txt file\n","    :return: None\n","    \"\"\"\n","\n","    f = open(path, \"w\")\n","    if save_raw:\n","        f.write(str(dic))\n","    else:\n","        for k, v in dic.items():\n","            f.write(str(k))\n","            f.write(str(v))\n","            f.write(\"\\n\\n\")\n","    f.close()\n","\n","\n","def save_exp_to_file(dic: dict, path: str) -> None:\n","    \"\"\"\n","    Save dict values into txt file\n","    :param dic: Dict with values\n","    :param path: Path to .txt file\n","    :return: None\n","    \"\"\"\n","\n","    f = open(path, \"a+\")\n","    keys = dic.keys()\n","    vals = [str(val) for val in dic.values()]\n","\n","    if f.tell() == 0:\n","        header = \"\\t\".join(keys)\n","        f.write(header + \"\\n\")\n","\n","    row = \"\\t\".join(vals)\n","    f.write(row + \"\\n\")\n","    f.close()\n","\n","\n","def cat_cols_info(\n","    X_train: pd.DataFrame, X_test: pd.DataFrame, cat_cols: List[str]\n",") -> dict:\n","    \"\"\"\n","    Get the main info about cat columns in dataframe, i.e. num of values, uniqueness\n","    :param X_train: Train dataframe\n","    :param X_test: Test dataframe\n","    :param cat_cols: List of categorical columns\n","    :return: Dict with results\n","    \"\"\"\n","\n","    cc_info = {}\n","\n","    for col in cat_cols:\n","        train_values = set(X_train[col])\n","        number_of_new_test = len(set(X_test[col]) - train_values)\n","        fraction_of_new_test = np.mean(\n","            X_test[col].apply(lambda v: v not in train_values)\n","        )\n","\n","        cc_info[col] = {\n","            \"num_uniq_train\": X_train[col].nunique(),\n","            \"num_uniq_test\": X_test[col].nunique(),\n","            \"number_of_new_test\": number_of_new_test,\n","            \"fraction_of_new_test\": fraction_of_new_test,\n","        }\n","    return cc_info\n","\n","\n","def adversarial_test(left_df, right_df, cat_cols):\n","    \"\"\"\n","    Trains adversarial model to distinguish train from test\n","    :param left_df:  dataframe\n","    :param right_df: dataframe\n","    :param cat_cols: List of categorical columns\n","    :return: trained model\n","    \"\"\"\n","    # sample to shuffle the data\n","    left_df = left_df.copy().sample(frac=1).reset_index(drop=True)\n","    right_df = right_df.copy().sample(frac=1).reset_index(drop=True)\n","\n","    left_df = left_df.head(right_df.shape[0])\n","    right_df = right_df.head(left_df.shape[0])\n","\n","    left_df[\"gt\"] = 0\n","    right_df[\"gt\"] = 1\n","\n","    concated = pd.concat([left_df, right_df])\n","    lgb_model = Model(\n","        cat_validation=\"Single\",\n","        encoders_names=(\"OrdinalEncoder\",),\n","        cat_cols=cat_cols,\n","        model_validation=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n","        model_params={\n","            \"metrics\": \"AUC\",\n","            \"max_depth\": 2,\n","            \"max_bin\": 100,\n","            \"n_estimators\": 500,\n","            \"learning_rate\": 0.02,\n","            \"random_state\": 42,\n","        },\n","    )\n","    train_score, val_score, avg_num_trees = lgb_model.fit(\n","        concated.drop(\"gt\", axis=1), concated[\"gt\"]\n","    )\n","\n","    print(\n","        \"ROC AUC adversarial: train %.2f%% val %.2f%%\"\n","        % (train_score * 100.0, val_score * 100.0)\n","    )\n","    return lgb_model\n","\n","\n","def extend_gan_train(x_train, y_train, x_test, cat_cols, gen_x_times=1.2, epochs=300):\n","    \"\"\"\n","    Extends train by generating new data by GAN\n","    :param x_train:  train dataframe\n","    :param y_train: target for train dataframe\n","    :param x_test: dataframe\n","    :param cat_cols: List of categorical columns\n","    :param gen_x_times: Factor for which initial dataframe should be increased\n","    :param cat_cols: List of categorical columns\n","    :param epochs: Number of epoch max to train the GAN\n","    :return: extended train with target\n","    \"\"\"\n","\n","    if gen_x_times == 0:\n","        raise ValueError(\"Passed gen_x_times with value 0!\")\n","    x_train[\"target\"] = y_train\n","    x_test_bigger = int(1.1 * x_test.shape[0] / x_train.shape[0])\n","    ctgan = _CTGANSynthesizer()\n","    ctgan.fit(x_train, cat_cols, epochs=epochs)\n","    generated_df = ctgan.sample((x_test_bigger) * x_train.shape[0])\n","    data_dtype = x_train.dtypes.values\n","\n","    for i in range(len(generated_df.columns)):\n","        generated_df[generated_df.columns[i]] = generated_df[\n","            generated_df.columns[i]\n","        ].astype(data_dtype[i])\n","\n","    generated_df = pd.concat(\n","        [\n","            x_train,\n","            generated_df,\n","        ]\n","    ).reset_index(drop=True)\n","\n","    num_cols = []\n","    for col in x_train.columns:\n","        if \"num\" in col:\n","            num_cols.append(col)\n","\n","    for num_col in num_cols:\n","        min_val = x_test[num_col].quantile(0.02)\n","        max_val = x_test[num_col].quantile(0.98)\n","        generated_df = generated_df.loc[\n","            (generated_df[num_col] >= min_val) & (generated_df[num_col] <= max_val)\n","        ]\n","    generated_df = generated_df.reset_index(drop=True)\n","    ad_model = adversarial_test(x_test, generated_df.drop(\"target\", axis=1), cat_cols)\n","\n","    generated_df[\"test_similarity\"] = ad_model.predict(\n","        generated_df.drop(\"target\", axis=1), return_shape=False\n","    )\n","    generated_df.sort_values(\"test_similarity\", ascending=False, inplace=True)\n","    generated_df = generated_df.head(int(gen_x_times * x_train.shape[0]))\n","    x_train = pd.concat(\n","        [x_train, generated_df.drop(\"test_similarity\", axis=1)], axis=0\n","    ).reset_index(drop=True)\n","    del generated_df\n","    gc.collect()\n","    return x_train.drop(\"target\", axis=1), x_train[\"target\"]\n","\n","\n","def extend_from_original(x_train, y_train, x_test, cat_cols, gen_x_times=1.2):\n","    \"\"\"\n","    Extends train by generating new data by GAN\n","    :param x_train:  train dataframe\n","    :param y_train: target for train dataframe\n","    :param x_test: test dataframe\n","    :param cat_cols: List of categorical columns\n","    :param gen_x_times: Factor for which initial dataframe should be increased\n","    :return: extended train with target\n","    \"\"\"\n","    if gen_x_times == 0:\n","        raise ValueError(\"Passed gen_x_times with value 0!\")\n","    x_train[\"target\"] = y_train\n","    x_test_bigger = int(gen_x_times * x_test.shape[0] / x_train.shape[0])\n","    generated_df = x_train.sample(frac=x_test_bigger, replace=True, random_state=42)\n","    num_cols = []\n","    for col in x_train.columns:\n","        if \"num\" in col:\n","            num_cols.append(col)\n","\n","    for num_col in num_cols:\n","        min_val = x_test[num_col].quantile(0.02)\n","        max_val = x_test[num_col].quantile(0.98)\n","        generated_df = generated_df.loc[\n","            (generated_df[num_col] >= min_val) & (generated_df[num_col] <= max_val)\n","        ]\n","\n","    generated_df = generated_df.reset_index(drop=True)\n","    ad_model = adversarial_test(x_test, generated_df.drop(\"target\", axis=1), cat_cols)\n","\n","    generated_df[\"test_similarity\"] = ad_model.predict(\n","        generated_df.drop(\"target\", axis=1), return_shape=False\n","    )\n","    generated_df.sort_values(\"test_similarity\", ascending=False, inplace=True)\n","    generated_df = generated_df.head(int(gen_x_times * x_train.shape[0]))\n","    x_train = pd.concat(\n","        [x_train, generated_df.drop(\"test_similarity\", axis=1)], axis=0\n","    ).reset_index(drop=True)\n","    del generated_df\n","    gc.collect()\n","    return x_train.drop(\"target\", axis=1), x_train[\"target\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M-yQ4lyooJMG"},"source":["import gc\n","import logging\n","from abc import ABC, abstractmethod\n","from typing import Tuple\n","\n","import pandas as pd\n","\n","__author__ = \"Insaf Ashrapov\"\n","__copyright__ = \"Insaf Ashrapov\"\n","__license__ = \"Apache 2.0\"\n","\n","\n","class SampleData(ABC):\n","    \"\"\"\n","        Factory method for different sampler strategies. The goal is to generate more train data\n","        which should be more close to test, in other word we trying to fix uneven distribution.\n","    \"\"\"\n","\n","    @abstractmethod\n","    def get_object_generator(self):\n","        \"\"\"\n","        Getter for object sampler aka generator, which is not a generator\n","        \"\"\"\n","        raise NotImplementedError\n","\n","    def generate_data_pipe(\n","        self,\n","        train_df: pd.DataFrame,\n","        target: pd.DataFrame,\n","        test_df: pd.DataFrame,\n","        deep_copy: bool = True,\n","        only_adversarial: bool = False,\n","        use_adversarial: bool = True,\n","        only_generated_data: bool = False,\n","    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n","        \"\"\"\n","        Defines logic for sampling\n","        @param train_df: Train dataframe which has separate target\n","        @param target: Input target for the train dataset\n","        @param test_df: Test dataframe - newly generated train dataframe should be close to it\n","        @param deep_copy: make copy of input files or not. If not input dataframes will be overridden\n","        @param only_adversarial: only adversarial fitering to train dataframe will be performed\n","        @param use_adversarial: perform or not adversarial filtering\n","        @param only_generated_data: After generation get only newly generated, without concating input train dataframe.\n","        Only works for SamplerGAN.\n","        @return: Newly generated train dataframe and test data\n","        \"\"\"\n","        generator = self.get_object_generator()\n","        if deep_copy:\n","            logging.info(\"Preprocessing input data with deep copying input data.\")\n","            if target is None or test_df is None:\n","                new_train = generator.preprocess_data_df(train_df.copy())\n","                new_target = None\n","            else:\n","                new_train, new_target, test_df = generator.preprocess_data(\n","                    train_df.copy(), target.copy(), test_df\n","                )\n","        else:\n","            logging.info(\"Preprocessing input data with deep copying input data.\")\n","            new_train, new_target, test_df = generator.preprocess_data(\n","                train_df, target, test_df\n","            )\n","        if only_adversarial and use_adversarial:\n","            logging.info(\"Applying adversarial filtering\")\n","            return generator.adversarial_filtering(new_train, new_target, test_df)\n","        else:\n","            logging.info(\"Starting generation step.\")\n","            new_train, new_target = generator.generate_data(\n","                new_train, new_target, test_df, only_generated_data\n","            )\n","            logging.info(\"Starting postprocessing step.\")\n","            new_train, new_target = generator.postprocess_data(\n","                new_train, new_target, test_df\n","            )\n","            if use_adversarial:\n","                logging.info(\"Applying adversarial filtering\")\n","                new_train, new_target = generator.adversarial_filtering(\n","                    new_train, new_target, test_df\n","                )\n","            gc.collect()\n","\n","            logging.info(\"Total finishing, returning data\")\n","            return new_train, new_target\n","\n","\n","class Sampler(ABC):\n","    \"\"\"\n","        Interface for each sampling strategy\n","    \"\"\"\n","\n","    def get_generated_shape(self, input_df):\n","        \"\"\"\n","        Calculates final output shape\n","        \"\"\"\n","        if self.gen_x_times <= 0:\n","            raise ValueError(\n","                \"Passed gen_x_times = {} should be bigger than 0\".format(\n","                    self.gen_x_times\n","                )\n","            )\n","        return int(self.gen_x_times * input_df.shape[0])\n","\n","    @abstractmethod\n","    def preprocess_data(self, train, target, test_df):\n","        \"\"\"Before we can start data generation we might need some preprocessing, numpy to pandas\n","        and etc\"\"\"\n","        raise NotImplementedError\n","\n","    @abstractmethod\n","    def generate_data(self, train_df, target, test_df):\n","        raise NotImplementedError\n","\n","    @abstractmethod\n","    def postprocess_data(self, train_df, target, test_df):\n","        \"\"\"Filtering data which far beyond from test_df data distribution\"\"\"\n","        raise NotImplementedError\n","\n","    @abstractmethod\n","    def adversarial_filtering(self, train_df, target, test_df):\n","        raise NotImplementedError\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fyn8uhHap-mG"},"source":["import logging\n","import sys\n","\n","\n","def setup_logging(loglevel):\n","    \"\"\"Setup basic logging\n","\n","    Args:\n","      loglevel (int): minimum loglevel for emitting messages\n","    \"\"\"\n","\n","    logformat = \"[%(asctime)s] %(levelname)s:%(name)s:%(message)s\"\n","    logging.basicConfig(\n","        level=loglevel, stream=sys.stdout, format=logformat, datefmt=\"%Y-%m-%d %H:%M:%S\"\n","    )\n","\n","\n","TEMP_TARGET = \"_temp_target\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TBJMz602qW5c"},"source":["%%capture\n","!pip install category_encoders"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BV15gSNyqezQ","executionInfo":{"status":"ok","timestamp":1636216368490,"user_tz":-60,"elapsed":992,"user":{"displayName":"Aurel Steve AVOMO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggi9E6HFsZM1Bm-x69aI0UuldSv9ATTh5Lrnn9KgQ=s64","userId":"07271015394144128293"}},"outputId":"a1ed0cc1-84fc-467f-9dfe-bd56b2968c98"},"source":["\n","from typing import List\n","\n","import numpy as np\n","import pandas as pd\n","from category_encoders.backward_difference import BackwardDifferenceEncoder\n","from category_encoders.cat_boost import CatBoostEncoder\n","from category_encoders.helmert import HelmertEncoder\n","from category_encoders.james_stein import JamesSteinEncoder\n","from category_encoders.leave_one_out import LeaveOneOutEncoder\n","from category_encoders.m_estimate import MEstimateEncoder\n","from category_encoders.one_hot import OneHotEncoder\n","from category_encoders.ordinal import OrdinalEncoder\n","from category_encoders.sum_coding import SumEncoder\n","from category_encoders.target_encoder import TargetEncoder\n","from category_encoders.woe import WOEEncoder\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","\n","\n","def get_single_encoder(encoder_name: str, cat_cols: list):\n","    \"\"\"\n","    Get encoder by its name\n","    :param encoder_name: Name of desired encoder\n","    :param cat_cols: Cat columns for encoding\n","    :return: Categorical encoder\n","    \"\"\"\n","    if encoder_name == \"FrequencyEncoder\":\n","        encoder = FrequencyEncoder(cols=cat_cols)\n","\n","    if encoder_name == \"WOEEncoder\":\n","        encoder = WOEEncoder(cols=cat_cols)\n","\n","    if encoder_name == \"TargetEncoder\":\n","        encoder = TargetEncoder(cols=cat_cols)\n","\n","    if encoder_name == \"SumEncoder\":\n","        encoder = SumEncoder(cols=cat_cols)\n","\n","    if encoder_name == \"MEstimateEncoder\":\n","        encoder = MEstimateEncoder(cols=cat_cols)\n","\n","    if encoder_name == \"LeaveOneOutEncoder\":\n","        encoder = LeaveOneOutEncoder(cols=cat_cols)\n","\n","    if encoder_name == \"HelmertEncoder\":\n","        encoder = HelmertEncoder(cols=cat_cols)\n","\n","    if encoder_name == \"BackwardDifferenceEncoder\":\n","        encoder = BackwardDifferenceEncoder(cols=cat_cols)\n","\n","    if encoder_name == \"JamesSteinEncoder\":\n","        encoder = JamesSteinEncoder(cols=cat_cols)\n","\n","    if encoder_name == \"OrdinalEncoder\":\n","        encoder = OrdinalEncoder(cols=cat_cols)\n","\n","    if encoder_name == \"CatBoostEncoder\":\n","        encoder = CatBoostEncoder(cols=cat_cols)\n","\n","    if encoder_name == \"MEstimateEncoder\":\n","        encoder = MEstimateEncoder(cols=cat_cols)\n","    if encoder_name == \"OneHotEncoder\":\n","        encoder = OneHotEncoder(cols=cat_cols)\n","    if encoder is None:\n","        raise NotImplementedError(\"To be implemented\")\n","    return encoder\n","\n","\n","class DoubleValidationEncoderNumerical:\n","    \"\"\"\n","    Encoder with validation within\n","    \"\"\"\n","\n","    def __init__(self, cols, encoders_names_tuple=()):\n","        \"\"\"\n","        :param cols: Categorical columns\n","        :param encoders_names_tuple: Tuple of str with encoders\n","        \"\"\"\n","        self.cols, self.num_cols = cols, None\n","        self.encoders_names_tuple = encoders_names_tuple\n","\n","        self.n_folds, self.n_repeats = 5, 3\n","        self.model_validation = RepeatedStratifiedKFold(\n","            n_splits=self.n_folds, n_repeats=self.n_repeats, random_state=0\n","        )\n","        self.encoders_dict = {}\n","\n","        self.storage = None\n","\n","    def fit_transform(self, X: pd.DataFrame, y: np.array) -> pd.DataFrame:\n","        self.num_cols = [col for col in X.columns if col not in self.cols]\n","        self.storage = []\n","\n","        for encoder_name in self.encoders_names_tuple:\n","            for n_fold, (train_idx, val_idx) in enumerate(\n","                self.model_validation.split(X, y)\n","            ):\n","                encoder = get_single_encoder(encoder_name, self.cols)\n","\n","                X_train, X_val = (\n","                    X.loc[train_idx].reset_index(drop=True),\n","                    X.loc[val_idx].reset_index(drop=True),\n","                )\n","                y_train, y_val = y[train_idx], y[val_idx]\n","                _ = encoder.fit_transform(X_train, y_train)\n","\n","                # transform validation part and get all necessary cols\n","                val_t = encoder.transform(X_val)\n","                val_t = val_t[\n","                    [col for col in val_t.columns if col not in self.num_cols]\n","                ].values\n","\n","                if encoder_name not in self.encoders_dict.keys():\n","                    cols_representation = np.zeros((X.shape[0], val_t.shape[1]))\n","                    self.encoders_dict[encoder_name] = [encoder]\n","                else:\n","                    self.encoders_dict[encoder_name].append(encoder)\n","\n","                cols_representation[val_idx, :] += val_t / self.n_repeats\n","\n","            cols_representation = pd.DataFrame(cols_representation)\n","            cols_representation.columns = [\n","                f\"encoded_{encoder_name}_{i}\"\n","                for i in range(cols_representation.shape[1])\n","            ]\n","            self.storage.append(cols_representation)\n","\n","        for df in self.storage:\n","            X = pd.concat([X, df], axis=1)\n","\n","        X.drop(self.cols, axis=1, inplace=True)\n","        return X\n","\n","    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n","        self.storage = []\n","        for encoder_name in self.encoders_names_tuple:\n","            cols_representation = None\n","\n","            for encoder in self.encoders_dict[encoder_name]:\n","                test_tr = encoder.transform(X)\n","                test_tr = test_tr[\n","                    [col for col in test_tr.columns if col not in self.num_cols]\n","                ].values\n","\n","                if cols_representation is None:\n","                    cols_representation = np.zeros(test_tr.shape)\n","\n","                cols_representation = (\n","                    cols_representation + test_tr / self.n_folds / self.n_repeats\n","                )\n","\n","            cols_representation = pd.DataFrame(cols_representation)\n","            cols_representation.columns = [\n","                f\"encoded_{encoder_name}_{i}\"\n","                for i in range(cols_representation.shape[1])\n","            ]\n","            self.storage.append(cols_representation)\n","\n","        for df in self.storage:\n","            X = pd.concat([X, df], axis=1)\n","\n","        X.drop(self.cols, axis=1, inplace=True)\n","        return X\n","\n","\n","class MultipleEncoder:\n","    \"\"\"\n","    Multiple encoder for categorical columns\n","    \"\"\"\n","\n","    def __init__(self, cols: List[str], encoders_names_tuple=()):\n","        \"\"\"\n","        :param cols: List of categorical columns\n","        :param encoders_names_tuple: Tuple of categorical encoders names. Possible values in tuple are:\n","        \"FrequencyEncoder\", \"WOEEncoder\", \"TargetEncoder\", \"SumEncoder\", \"MEstimateEncoder\", \"LeaveOneOutEncoder\",\n","        \"HelmertEncoder\", \"BackwardDifferenceEncoder\", \"JamesSteinEncoder\", \"OrdinalEncoder\"\"CatBoostEncoder\"\n","        \"\"\"\n","\n","        self.cols = cols\n","        self.num_cols = None\n","        self.encoders_names_tuple = encoders_names_tuple\n","        self.encoders_dict = {}\n","\n","        # list for storing results of transformation from each encoder\n","        self.storage = None\n","\n","    def fit_transform(self, X: pd.DataFrame, y: np.array) -> pd.DataFrame:\n","        self.num_cols = [col for col in X.columns if col not in self.cols]\n","        self.storage = []\n","        for encoder_name in self.encoders_names_tuple:\n","            encoder = get_single_encoder(encoder_name=encoder_name, cat_cols=self.cols)\n","\n","            cols_representation = encoder.fit_transform(X, y)\n","            self.encoders_dict[encoder_name] = encoder\n","            cols_representation = cols_representation[\n","                [col for col in cols_representation.columns if col not in self.num_cols]\n","            ].values\n","            cols_representation = pd.DataFrame(cols_representation)\n","            cols_representation.columns = [\n","                f\"encoded_{encoder_name}_{i}\"\n","                for i in range(cols_representation.shape[1])\n","            ]\n","            self.storage.append(cols_representation)\n","\n","        # concat cat cols representations with initial dataframe\n","        for df in self.storage:\n","            X = pd.concat([X, df], axis=1)\n","\n","        # remove all columns as far as we have their representations\n","        X.drop(self.cols, axis=1, inplace=True)\n","        return X\n","\n","    def transform(self, X) -> pd.DataFrame:\n","        self.storage = []\n","        for encoder_name in self.encoders_names_tuple:\n","            # get representation of cat columns and form a pd.DataFrame for it\n","            cols_representation = self.encoders_dict[encoder_name].transform(X)\n","            cols_representation = cols_representation[\n","                [col for col in cols_representation.columns if col not in self.num_cols]\n","            ].values\n","            cols_representation = pd.DataFrame(cols_representation)\n","            cols_representation.columns = [\n","                f\"encoded_{encoder_name}_{i}\"\n","                for i in range(cols_representation.shape[1])\n","            ]\n","            self.storage.append(cols_representation)\n","\n","        # concat cat cols representations with initial dataframe\n","        for df in self.storage:\n","            X = pd.concat([X, df], axis=1)\n","\n","        # remove all columns as far as we have their representations\n","        X.drop(self.cols, axis=1, inplace=True)\n","        return X\n","\n","\n","class FrequencyEncoder:\n","    def __init__(self, cols):\n","        self.cols = cols\n","        self.counts_dict = None\n","\n","    def fit(self, X: pd.DataFrame):\n","        counts_dict = {}\n","        for col in self.cols:\n","            values, counts = np.unique(X[col], return_counts=True)\n","            counts_dict[col] = dict(zip(values, counts))\n","        self.counts_dict = counts_dict\n","\n","    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n","        counts_dict_test = {}\n","        res = []\n","        for col in self.cols:\n","            values, counts = np.unique(X[col], return_counts=True)\n","            counts_dict_test[col] = dict(zip(values, counts))\n","\n","            # if value is in \"train\" keys - replace \"test\" counts with \"train\" counts\n","            for k in [\n","                key\n","                for key in counts_dict_test[col].keys()\n","                if key in self.counts_dict[col].keys()\n","            ]:\n","                counts_dict_test[col][k] = self.counts_dict[col][k]\n","\n","            res.append(X[col].map(counts_dict_test[col]).values.reshape(-1, 1))\n","        res = np.hstack(res)\n","\n","        X[self.cols] = res\n","        return X\n","\n","    def fit_transform(self, X: pd.DataFrame, y=None) -> pd.DataFrame:\n","        self.fit(X, y)\n","        X = self.transform(X)\n","        return X\n","\n","\n","if __name__ == \"__main__\":\n","    df = pd.DataFrame({})\n","    df[\"cat_col\"] = [1, 2, 3, 1, 2, 3, 1, 1, 1]\n","    df[\"target\"] = [0, 1, 0, 1, 0, 1, 0, 1, 0]\n","\n","    #\n","    temp = df.copy()\n","    enc = CatBoostEncoder(cols=[\"cat_col\"])\n","    print(enc.fit_transform(temp, temp[\"target\"]))\n","\n","    #\n","    temp = df.copy()\n","    enc = MultipleEncoder(cols=[\"cat_col\"], encoders_names_tuple=(\"CatBoostEncoder\",))\n","    print(enc.fit_transform(temp, temp[\"target\"]))\n","\n","    #\n","    temp = df.copy()\n","    enc = DoubleValidationEncoderNumerical(\n","        cols=[\"cat_col\"], encoders_names_tuple=(\"CatBoostEncoder\",)\n","    )\n","    print(enc.fit_transform(temp, temp[\"target\"]))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["    cat_col  target\n","0  0.444444       0\n","1  0.444444       1\n","2  0.444444       0\n","3  0.222222       1\n","4  0.722222       0\n","5  0.222222       1\n","6  0.481481       0\n","7  0.361111       1\n","8  0.488889       0\n","   target  encoded_CatBoostEncoder_0\n","0       0                   0.444444\n","1       1                   0.444444\n","2       0                   0.444444\n","3       1                   0.222222\n","4       0                   0.722222\n","5       1                   0.222222\n","6       0                   0.481481\n","7       1                   0.361111\n","8       0                   0.488889\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n","  % (min_groups, self.n_splits)), UserWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n","  % (min_groups, self.n_splits)), UserWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n","  % (min_groups, self.n_splits)), UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["   target  encoded_CatBoostEncoder_0\n","0       0                   0.447619\n","1       1                   0.428571\n","2       0                   0.428571\n","3       1                   0.357143\n","4       0                   0.452381\n","5       1                   0.428571\n","6       0                   0.357143\n","7       1                   0.357143\n","8       0                   0.404762\n"]}]},{"cell_type":"code","metadata":{"id":"EyBYs5nQoKTT"},"source":["import numpy as np\n","import pandas as pd\n","from lightgbm import LGBMClassifier\n","from scipy.stats import rankdata\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import StratifiedKFold\n","\n","\n","\n","class AdversarialModel:\n","    def __init__(\n","            self,\n","            cat_validation=\"Single\",\n","            encoders_names=(\"OrdinalEncoder\",),\n","            cat_cols=None,\n","            model_validation=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n","            model_params=None,\n","    ):\n","        '''\n","        Class for fit predicting tabular models, mostly - boostings. Several encoders for categorical features are\n","        supported\n","\n","        Args:\n","            cat_validation: categorical type of validation, examples: \"None\", \"Single\" and \"Double\"\n","            encoders_names: different categorical encoders from category_encoders library, example CatBoostEncoder\n","            cat_cols: list of categorical columns\n","            model_validation: model training cross validation type from sklearn.model_selection,\n","            example StratifiedKFold(5)\n","            model_params: model training hyperparameters\n","        '''\n","        self.cat_validation = cat_validation\n","        self.encoders_names = encoders_names\n","        self.cat_cols = cat_cols\n","        self.model_validation = model_validation\n","        self.model_params = model_params\n","\n","    def adversarial_test(self, left_df, right_df):\n","        \"\"\"\n","        Trains adversarial model to distinguish train from test\n","        :param left_df:  dataframe\n","        :param right_df: dataframe\n","        :return: trained model\n","        \"\"\"\n","        # sample to shuffle the data\n","        left_df = left_df.copy().sample(frac=1).reset_index(drop=True)\n","        right_df = right_df.copy().sample(frac=1).reset_index(drop=True)\n","\n","        left_df = left_df.head(right_df.shape[0])\n","        right_df = right_df.head(left_df.shape[0])\n","\n","        left_df[\"gt\"] = 0\n","        right_df[\"gt\"] = 1\n","\n","        concated = pd.concat([left_df, right_df])\n","        lgb_model = Model(\n","            cat_validation=self.cat_validation,\n","            encoders_names=self.encoders_names,\n","            cat_cols=self.cat_cols,\n","            model_validation=self.model_validation,\n","            model_params=self.model_params,\n","        )\n","        train_score, val_score, avg_num_trees = lgb_model.fit(\n","            concated.drop(\"gt\", axis=1), concated[\"gt\"]\n","        )\n","        self.metrics = {\"train_score\": train_score,\n","                        \"val_score\": val_score,\n","                        \"avg_num_trees\": avg_num_trees}\n","        self.trained_model = lgb_model\n","\n","\n","class Model:\n","    def __init__(\n","            self,\n","            cat_validation=\"None\",\n","            encoders_names=None,\n","            cat_cols=None,\n","            model_validation=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n","            model_params=None,\n","    ):\n","        '''\n","        Class for fit predicting tabular models, mostly - boostings. Several encoders for categorical features are supported\n","\n","        Args:\n","            cat_validation: categorical type of validation, examples: \"None\", \"Single\" and \"Double\"\n","            encoders_names: different categorical encoders from category_encoders library, example CatBoostEncoder\n","            cat_cols: list of categorical columns\n","            model_validation: model training cross validation type from sklearn.model_selection, example StratifiedKFold(5)\n","            model_params: model training hyperparameters\n","        '''\n","        self.cat_validation = cat_validation\n","        self.encoders_names = encoders_names\n","        self.cat_cols = cat_cols\n","        self.model_validation = model_validation\n","\n","        if model_params is None:\n","            self.model_params = {\n","                \"metrics\": \"AUC\",\n","                \"n_estimators\": 5000,\n","                \"learning_rate\": 0.04,\n","                \"random_state\": 42,\n","            }\n","        else:\n","            self.model_params = model_params\n","\n","        self.encoders_list = []\n","        self.models_list = []\n","        self.scores_list_train = []\n","        self.scores_list_val = []\n","        self.models_trees = []\n","\n","    def fit(self, X: pd.DataFrame, y: np.array) -> tuple:\n","        \"\"\"\n","        Fits model with speficified in init params\n","        Args:\n","            X: Input training dataframe\n","            y: Target for X\n","\n","        Returns:\n","            mean_score_train, mean_score_val, avg_num_trees\n","        \"\"\"\n","        # process cat cols\n","        if self.cat_validation == \"None\":\n","            encoder = MultipleEncoder(\n","                cols=self.cat_cols, encoders_names_tuple=self.encoders_names\n","            )\n","            X = encoder.fit_transform(X, y)\n","\n","        for n_fold, (train_idx, val_idx) in enumerate(\n","                self.model_validation.split(X, y)\n","        ):\n","            X_train, X_val = (\n","                X.iloc[train_idx].reset_index(drop=True),\n","                X.iloc[val_idx].reset_index(drop=True),\n","            )\n","            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n","            if self.cat_cols is not None:\n","                if self.cat_validation == \"Single\":\n","                    encoder = MultipleEncoder(\n","                        cols=self.cat_cols, encoders_names_tuple=self.encoders_names\n","                    )\n","                    X_train = encoder.fit_transform(X_train, y_train)\n","                    X_val = encoder.transform(X_val)\n","                if self.cat_validation == \"Double\":\n","                    encoder = DoubleValidationEncoderNumerical(\n","                        cols=self.cat_cols, encoders_names_tuple=self.encoders_names\n","                    )\n","                    X_train = encoder.fit_transform(X_train, y_train)\n","                    X_val = encoder.transform(X_val)\n","                self.encoders_list.append(encoder)\n","\n","                # check for OrdinalEncoder encoding\n","                for col in [col for col in X_train.columns if \"OrdinalEncoder\" in col]:\n","                    X_train[col] = X_train[col].astype(\"category\")\n","                    X_val[col] = X_val[col].astype(\"category\")\n","\n","            # fit model\n","            model = LGBMClassifier(**self.model_params)\n","            model.fit(\n","                X_train,\n","                y_train,\n","                eval_set=[(X_train, y_train), (X_val, y_val)],\n","                early_stopping_rounds=50,\n","                verbose=False,\n","            )\n","            self.models_trees.append(model.best_iteration_)\n","            self.models_list.append(model)\n","\n","            y_hat = model.predict_proba(X_train)[:, 1]\n","            score_train = roc_auc_score(y_train, y_hat)\n","            self.scores_list_train.append(score_train)\n","            y_hat = model.predict_proba(X_val)[:, 1]\n","            score_val = roc_auc_score(y_val, y_hat)\n","            self.scores_list_val.append(score_val)\n","\n","        mean_score_train = np.mean(self.scores_list_train)\n","        mean_score_val = np.mean(self.scores_list_val)\n","        avg_num_trees = int(np.mean(self.models_trees))\n","\n","        return mean_score_train, mean_score_val, avg_num_trees\n","\n","    def predict(self, X: pd.DataFrame) -> np.array:\n","        \"\"\"\n","        Making inference with trained models for input dataframe\n","        Args:\n","            X: input dataframe for inference\n","\n","        Returns: Predicted ranks\n","\n","        \"\"\"\n","        y_hat = np.zeros(X.shape[0])\n","        if self.encoders_list is not None and self.encoders_list != []:\n","            for encoder, model in zip(self.encoders_list, self.models_list):\n","                X_test = X.copy()\n","                X_test = encoder.transform(X_test)\n","\n","                # check for OrdinalEncoder encoding\n","                for col in [col for col in X_test.columns if \"OrdinalEncoder\" in col]:\n","                    X_test[col] = X_test[col].astype(\"category\")\n","\n","                unranked_preds = model.predict_proba(X_test)[:, 1]\n","                y_hat += rankdata(unranked_preds)\n","        else:\n","            for model in self.models_list:\n","                X_test = X.copy()\n","\n","                unranked_preds = model.predict_proba(X_test)[:, 1]\n","                y_hat += rankdata(unranked_preds)\n","        return y_hat\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":120},"id":"XSQUVFh9nm4m","executionInfo":{"status":"ok","timestamp":1636216371218,"user_tz":-60,"elapsed":548,"user":{"displayName":"Aurel Steve AVOMO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggi9E6HFsZM1Bm-x69aI0UuldSv9ATTh5Lrnn9KgQ=s64","userId":"07271015394144128293"}},"outputId":"2ffe9c8d-07b4-4ad3-e6ce-89231e71e2e0"},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","todo write description\n","\"\"\"\n","\n","import gc\n","import logging\n","import warnings\n","from typing import Tuple\n","\n","import numpy as np\n","import pandas as pd\n","\n","\n","\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","__author__ = \"Insaf Ashrapov\"\n","__copyright__ = \"Insaf Ashrapov\"\n","__license__ = \"Apache 2.0\"\n","\n","__all__ = [\"OriginalGenerator\", \"GANGenerator\"]\n","\n","\n","class OriginalGenerator(SampleData):\n","    def __init__(self, *args, **kwargs):\n","        self.args = args\n","        self.kwargs = kwargs\n","\n","    def get_object_generator(self) -> Sampler:\n","        return SamplerOriginal(*self.args, **self.kwargs)\n","\n","\n","class GANGenerator(SampleData):\n","    def __init__(self, *args, **kwargs):\n","        self.args = args\n","        self.kwargs = kwargs\n","\n","    def get_object_generator(self) -> Sampler:\n","        return SamplerGAN(*self.args, **self.kwargs)\n","\n","\n","class SamplerOriginal(Sampler):\n","    def __init__(\n","        self,\n","        gen_x_times: float = 1.1,\n","        cat_cols: list = None,\n","        bot_filter_quantile: float = 0.001,\n","        top_filter_quantile: float = 0.999,\n","        is_post_process: bool = True,\n","        adversaial_model_params: dict = {\n","            \"metrics\": \"AUC\",\n","            \"max_depth\": 2,\n","            \"max_bin\": 100,\n","            \"n_estimators\": 500,\n","            \"learning_rate\": 0.02,\n","            \"random_state\": 42,\n","        },\n","        pregeneration_frac: float = 2,\n","        only_generated_data: bool = False,\n","        gan_params: dict = {'batch_size': 500, 'patience': 25, \"epochs\" : 500,}\n","    ):\n","        \"\"\"\n","\n","        @param gen_x_times: float = 1.1 - how much data to generate, output might be less because of postprocessing and\n","        adversarial filtering\n","        @param cat_cols: list = None - categorical columns\n","        @param bot_filter_quantile: float = 0.001 - bottom quantile for postprocess filtering\n","        @param top_filter_quantile: float = 0.999 - bottom quantile for postprocess filtering\n","        @param is_post_process: bool = True - perform or not postfiltering, if false bot_filter_quantile\n","         and top_filter_quantile ignored\n","        @param adversarial_model_params: dict params for adversarial filtering model, default values for binary task\n","        @param pregeneration_frac: float = 2 - for generation step gen_x_times * pregeneration_frac amount of data\n","        will generated. However in postprocessing (1 + gen_x_times) % of original data will be returned\n","        @param only_generated_data: bool = False If True after generation get only newly generated, without concating input train dataframe.\n","        @param gan_params: dict params for GAN training\n","        Only works for SamplerGAN.\n","        \"\"\"\n","        self.gen_x_times = gen_x_times\n","        self.cat_cols = cat_cols\n","        self.is_post_process = is_post_process\n","        self.bot_filter_quantile = bot_filter_quantile\n","        self.top_filter_quantile = top_filter_quantile\n","        self.adversarial_model_params = adversaial_model_params\n","        self.pregeneration_frac = pregeneration_frac\n","        self.only_generated_data = only_generated_data\n","        self.gan_params = gan_params\n","        self.TEMP_TARGET = \"TEMP_TARGET\"\n","\n","    def preprocess_data_df(self, df) -> pd.DataFrame:\n","        logging.info(\"Input shape: {}\".format(df.shape))\n","        if isinstance(df, pd.DataFrame) is False:\n","            raise ValueError(\n","                \"Input dataframe aren't pandas dataframes: df is {}\".format(type(df))\n","            )\n","        return df\n","\n","    def preprocess_data(\n","        self, train, target, test_df\n","    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n","        train = self.preprocess_data_df(train)\n","        target = self.preprocess_data_df(target)\n","        test_df = self.preprocess_data_df(test_df)\n","        self.TEMP_TARGET = target.columns[0]\n","        if self.TEMP_TARGET in train.columns:\n","            raise ValueError(\n","                \"Input train dataframe already have {} column, consider removing it\".format(\n","                    self.TEMP_TARGET\n","                )\n","            )\n","        if \"test_similarity\" in train.columns:\n","            raise ValueError(\n","                \"Input train dataframe already have test_similarity, consider removing it\"\n","            )\n","\n","        return train, target, test_df\n","\n","    def generate_data(\n","        self, train_df, target, test_df, only_generated_data\n","    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n","        if only_generated_data:\n","            Warning.warn(\n","                \"For SamplerOriginal setting only_generated_data doesn't change anything, \"\n","                \"because generated data sampled from the train!\"\n","            )\n","        self._validate_data(train_df, target, test_df)\n","        train_df[self.TEMP_TARGET] = target\n","        generated_df = train_df.sample(\n","            frac=(1 + self.pregeneration_frac), replace=True, random_state=42\n","        )\n","        generated_df = generated_df.reset_index(drop=True)\n","        gc.collect()\n","        logging.info(\n","            \"Generated shape: {} and {}\".format(\n","                generated_df.drop(self.TEMP_TARGET, axis=1).shape,\n","                generated_df[self.TEMP_TARGET].shape,\n","            )\n","        )\n","        return (\n","            generated_df.drop(self.TEMP_TARGET, axis=1),\n","            generated_df[self.TEMP_TARGET],\n","        )\n","\n","    def postprocess_data(self, train_df, target, test_df):\n","        if not self.is_post_process or test_df is None:\n","            logging.info(\"Skipping postprocessing\")\n","            return train_df, target\n","\n","        self._validate_data(train_df, target, test_df)\n","        train_df[self.TEMP_TARGET] = target\n","\n","        for num_col in test_df.columns:\n","            if self.cat_cols is None or num_col not in self.cat_cols:\n","                min_val = test_df[num_col].quantile(self.bot_filter_quantile)\n","                max_val = test_df[num_col].quantile(self.top_filter_quantile)\n","                filtered_df = train_df.loc[\n","                    (train_df[num_col] >= min_val) & (train_df[num_col] <= max_val)\n","                ]\n","                if filtered_df.shape[0] < 10:\n","                    raise ValueError(\n","                        \"After post-processing generated data's shape less than 10. For columns {} test \"\n","                        \"might be highly skewed. Filter conditions are min_val = {} and max_val = {}.\".format(\n","                            num_col, min_val, max_val\n","                        )\n","                    )\n","                train_df = filtered_df\n","\n","        if self.cat_cols is not None:\n","            for cat_col in self.cat_cols:\n","                filtered_df = train_df[\n","                    train_df[cat_col].isin(test_df[cat_col].unique())\n","                ]\n","                if filtered_df.shape[0] < 10:\n","                    raise ValueError(\n","                        \"After post-processing generated data's shape less than 10. For columns {} test \"\n","                        \"might be highly skewed.\".format(num_col)\n","                    )\n","                train_df = filtered_df\n","        gc.collect()\n","        logging.info(\n","            \"Generated shapes after postprocessing: {} plus target\".format(\n","                train_df.drop(self.TEMP_TARGET, axis=1).shape\n","            )\n","        )\n","        return (\n","            train_df.drop(self.TEMP_TARGET, axis=1).reset_index(drop=True),\n","            train_df[self.TEMP_TARGET].reset_index(drop=True),\n","        )\n","\n","    def adversarial_filtering(self, train_df, target, test_df):\n","        if test_df is None:\n","            logging.info(\"Skipping adversarial filtering, because test_df is None.\")\n","            return train_df, target\n","        ad_model = AdversarialModel(\n","            cat_cols=self.cat_cols, model_params=self.adversarial_model_params\n","        )\n","        self._validate_data(train_df, target, test_df)\n","        train_df[self.TEMP_TARGET] = target\n","        ad_model.adversarial_test(test_df, train_df.drop(self.TEMP_TARGET, axis=1))\n","\n","        train_df[\"test_similarity\"] = ad_model.trained_model.predict(\n","            train_df.drop(self.TEMP_TARGET, axis=1)\n","        )\n","        train_df.sort_values(\"test_similarity\", ascending=False, inplace=True)\n","        train_df = train_df.head(self.get_generated_shape(train_df) * train_df.shape[0])\n","        del ad_model\n","        gc.collect()\n","        return (\n","            train_df.drop([\"test_similarity\", self.TEMP_TARGET], axis=1).reset_index(\n","                drop=True\n","            ),\n","            train_df[self.TEMP_TARGET].reset_index(drop=True),\n","        )\n","\n","    @staticmethod\n","    def _validate_data(train_df, target, test_df):\n","        if test_df is not None:\n","            if train_df.shape[0] < 10 or test_df.shape[0] < 10:\n","                raise ValueError(\n","                    \"Shape of train is {} and test is {}. Both should at least 10! \"\n","                    \"Consider disabling adversarial filtering\".format(\n","                        train_df.shape[0], test_df.shape[0]\n","                    )\n","                )\n","        if target is not None:\n","            if train_df.shape[0] != target.shape[0]:\n","                raise ValueError(\n","                    \"Something gone wrong: shape of train_df = {} is not equal to target = {} shape\".format(\n","                        train_df.shape[0], target.shape[0]\n","                    )\n","                )\n","\n","\n","class SamplerGAN(SamplerOriginal):\n","    def generate_data(\n","        self, train_df, target, test_df, only_generated_data: bool\n","    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n","        self._validate_data(train_df, target, test_df)\n","        if target is not None:\n","            train_df[self.TEMP_TARGET] = target\n","        ctgan = CTGANSynthesizer(batch_size=self.gan_params[\"batch_size\"], patience=self.gan_params[\"patience\"])\n","        logging.info(\"training GAN\")\n","        if self.cat_cols is None:\n","            ctgan.fit(train_df, [], epochs=self.gan_params[\"epochs\"])\n","        else:\n","            ctgan.fit(train_df, self.cat_cols, epochs=self.gan_params[\"epochs\"])\n","        logging.info(\"Finished training GAN\")\n","        generated_df = ctgan.sample(\n","            self.pregeneration_frac * self.get_generated_shape(train_df)\n","        )\n","        data_dtype = train_df.dtypes.values\n","\n","        for i in range(len(generated_df.columns)):\n","            generated_df[generated_df.columns[i]] = generated_df[\n","                generated_df.columns[i]\n","            ].astype(data_dtype[i])\n","        gc.collect()\n","        if not only_generated_data:\n","            train_df = pd.concat([train_df, generated_df]).reset_index(drop=True)\n","            logging.info(\n","                \"Generated shapes: {} plus target\".format(\n","                    _drop_col_if_exist(train_df, self.TEMP_TARGET).shape\n","                )\n","            )\n","            return (\n","                _drop_col_if_exist(train_df, self.TEMP_TARGET),\n","                get_columns_if_exists(train_df, self.TEMP_TARGET),\n","            )\n","        else:\n","            logging.info(\n","                \"Generated shapes: {} plus target\".format(\n","                    _drop_col_if_exist(train_df, self.TEMP_TARGET).shape\n","                )\n","            )\n","            return (\n","                _drop_col_if_exist(generated_df, self.TEMP_TARGET),\n","                get_columns_if_exists(generated_df, self.TEMP_TARGET),\n","            )\n","        gc.collect()\n","\n","        return (\n","            _drop_col_if_exist(train_df, self.TEMP_TARGET),\n","            get_columns_if_exists(train_df, self.TEMP_TARGET),\n","        )\n","\n","\n","def _sampler(creator: SampleData, in_train, in_target, in_test) -> None:\n","    _logger = logging.getLogger(__name__)\n","    _logger.info(\"Starting generating data\")\n","    train, test = creator.generate_data_pipe(in_train, in_target, in_test)\n","    _logger.info(train, test)\n","    _logger.info(\"Finished generation\\n\")\n","    return train, test\n","\n","\n","def _drop_col_if_exist(df, col_to_drop) -> pd.DataFrame:\n","    \"\"\"\n","    Drops col_to_drop from input dataframe df if sucj column exists\n","    \"\"\"\n","    if col_to_drop in df.columns:\n","        return df.drop(col_to_drop, axis=1)\n","    else:\n","        return df\n","\n","\n","def get_columns_if_exists(df, col) -> pd.DataFrame:\n","    if col in df.columns:\n","        return df[col]\n","    else:\n","        return None\n","\n","\"\"\"\n","if __name__ == \"__main__\":\n","    setup_logging(logging.DEBUG)\n","    train = pd.DataFrame(\n","        np.random.randint(-10, 150, size=(100, 4)), columns=list(\"ABCD\")\n","    )\n","    logging.info(train)\n","    target = pd.DataFrame(np.random.randint(0, 2, size=(100, 1)), columns=list(\"Y\"))\n","    test = pd.DataFrame(np.random.randint(0, 100, size=(100, 4)), columns=list(\"ABCD\"))\n","    _sampler(OriginalGenerator(gen_x_times=15), train, target, test)\n","    _sampler(\n","        GANGenerator(gen_x_times=10, only_generated_data=False,\n","                     gan_params={\"batch_size\": 500, \"patience\": 25, \"epochs\" : 500,}), train, target, test\n","    )\n","\n","    _sampler(OriginalGenerator(gen_x_times=15), train, None, train)\n","    _sampler(\n","        GANGenerator(cat_cols=[\"A\"], gen_x_times=20, only_generated_data=True),\n","        train,\n","        None,\n","        train,\n","    )\n","\"\"\""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nif __name__ == \"__main__\":\\n    setup_logging(logging.DEBUG)\\n    train = pd.DataFrame(\\n        np.random.randint(-10, 150, size=(100, 4)), columns=list(\"ABCD\")\\n    )\\n    logging.info(train)\\n    target = pd.DataFrame(np.random.randint(0, 2, size=(100, 1)), columns=list(\"Y\"))\\n    test = pd.DataFrame(np.random.randint(0, 100, size=(100, 4)), columns=list(\"ABCD\"))\\n    _sampler(OriginalGenerator(gen_x_times=15), train, target, test)\\n    _sampler(\\n        GANGenerator(gen_x_times=10, only_generated_data=False,\\n                     gan_params={\"batch_size\": 500, \"patience\": 25, \"epochs\" : 500,}), train, target, test\\n    )\\n\\n    _sampler(OriginalGenerator(gen_x_times=15), train, None, train)\\n    _sampler(\\n        GANGenerator(cat_cols=[\"A\"], gen_x_times=20, only_generated_data=True),\\n        train,\\n        None,\\n        train,\\n    )\\n'"]},"metadata":{},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"d62y-oejr3-I"},"source":["## Model testing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":159},"id":"RgCqR_tWr027","executionInfo":{"status":"ok","timestamp":1636216448335,"user_tz":-60,"elapsed":30780,"user":{"displayName":"Aurel Steve AVOMO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggi9E6HFsZM1Bm-x69aI0UuldSv9ATTh5Lrnn9KgQ=s64","userId":"07271015394144128293"}},"outputId":"018f03bf-bba6-47e1-b34e-d49b5e8ff018"},"source":["gen = GANGenerator(gan_params = {\"batch_size\": 100,\n","                                 \"patience\": 25, \"epochs\" : 100}).generate_data_pipe(train_data, None,None)\n","\n","calculate_metrics(train_data, gen[0], n_time=100)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GAN: Early stopping after epochs 47\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ander</th>\n","      <th>kendall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>real</th>\n","      <td>1.006206</td>\n","      <td>0.010928</td>\n","    </tr>\n","    <tr>\n","      <th>random</th>\n","      <td>13.797143</td>\n","      <td>0.125817</td>\n","    </tr>\n","    <tr>\n","      <th>model</th>\n","      <td>20.000198</td>\n","      <td>0.089588</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            ander   kendall\n","real     1.006206  0.010928\n","random  13.797143  0.125817\n","model   20.000198  0.089588"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","metadata":{"id":"wh-FJfXhykcB"},"source":["def calculate_metrics(real_data, generate_data, size = 411,n_time = 20, mean = 0.01, mu = 0.01):\n","  random_list, real_list, generated_list = [], [], []\n","\n","  for i in range(n_time):\n","    real_true = real_data.sample(size).values\n","    real_test = real_data.sample(size).values\n","    random = np.abs(np.random.normal(mean, mu, (size, 4)))\n","    generated = generate_data.sample(size).values\n","\n","    #true data\n","    real_list.append({'ander': anderson_evaluation(real_true, real_test), 'kendall' : kendall_evaluation(real_true, real_test)})\n","\n","    #random data\n","    random_list.append({'ander': anderson_evaluation(real_true,random), 'kendall' : kendall_evaluation(real_true, random)})\n","\n","    #generate\n","    generated_list.append({'ander': anderson_evaluation(real_true, generated), 'kendall' : kendall_evaluation(real_true, generated)})\n","  \n","\n","  random_df = pd.DataFrame(random_list)\n","  real_df = pd.DataFrame(real_list)\n","  generated_df = pd.DataFrame(generated_list)\n","\n","  liste = [ {'ander': real_df['ander'].mean(), 'kendall': real_df['kendall'].mean()}, {'ander': random_df['ander'].mean(), 'kendall': random_df['kendall'].mean()}, \n","            {'ander': generated_df['ander'].mean(), 'kendall': generated_df['kendall'].mean()} ]\n","\n","  return pd.DataFrame(liste, index=['real', 'random', 'model'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":142},"id":"VmiEiignB3i6","executionInfo":{"status":"ok","timestamp":1636216414675,"user_tz":-60,"elapsed":4081,"user":{"displayName":"Aurel Steve AVOMO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggi9E6HFsZM1Bm-x69aI0UuldSv9ATTh5Lrnn9KgQ=s64","userId":"07271015394144128293"}},"outputId":"12900d9e-7269-454f-b9f3-cea5c060745a"},"source":["calculate_metrics(train_data, gen[0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ander</th>\n","      <th>kendall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>real</th>\n","      <td>0.911547</td>\n","      <td>0.012345</td>\n","    </tr>\n","    <tr>\n","      <th>random</th>\n","      <td>13.911728</td>\n","      <td>0.125293</td>\n","    </tr>\n","    <tr>\n","      <th>model</th>\n","      <td>69.449626</td>\n","      <td>0.084488</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            ander   kendall\n","real     0.911547  0.012345\n","random  13.911728  0.125293\n","model   69.449626  0.084488"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QpTiikCzCDjA","executionInfo":{"status":"ok","timestamp":1636216414678,"user_tz":-60,"elapsed":40,"user":{"displayName":"Aurel Steve AVOMO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggi9E6HFsZM1Bm-x69aI0UuldSv9ATTh5Lrnn9KgQ=s64","userId":"07271015394144128293"}},"outputId":"3f816287-4b47-4fee-ccbd-c023f4c8bf1c"},"source":["np.abs(np.random.normal(0.01, 0.01, (4,2)))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.01772611, 0.00212386],\n","       [0.00218133, 0.00489999],\n","       [0.00883648, 0.00984078],\n","       [0.01642336, 0.025435  ]])"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","metadata":{"id":"iDymsUsPRlsK"},"source":[""],"execution_count":null,"outputs":[]}]}